import re



def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    # text = ' '.join(text.split())
    if not isinstance(text, str):
        raise TypeError("text must be a str")
    text = (
        text.replace("\\r\\n", " ")
        .replace("\\n", " ")
        .replace("\\r", " ")
        .replace("\\t", " ")
    )

    text = (
        text.replace("\r\n", " ")
        .replace("\r", " ")
        .replace("\n", " ")
        .replace("\t", " ")
    )

    text = re.sub(r"\s+", " ", text).strip()
    if casefold:
        text = text.casefold()
    if "Ñ‘" not in text:
        yo2e = False
    if yo2e:
        text = text.replace("Ñ‘", "Ðµ")
    return text


"""
for el in test1:
    print(f'{repr(el)} -> {repr(normalize(el))}')
"""
test2 = [
    "Ð¿Ñ€Ð¸Ð²ÐµÑ‚ Ð¼Ð¸Ñ€",
    "hello,world!!!",
    "2025 Ð³Ð¾Ð´",
    "Ð¿Ð¾-Ð½Ð°ÑÑ‚Ð¾ÑÑ‰ÐµÐ¼Ñƒ ÐºÑ€ÑƒÑ‚Ð¾",
    "emoji ðŸ˜€ Ð½Ðµ ÑÐ»Ð¾Ð²Ð¾",
]


def tokenize(text: str) -> list[str]:
    return re.findall(r"[Ð°-Ña-z0-9-_]+", text)


"""
for el in test2:
    print(f'{repr(el)} -> {repr(tokenize(el))}')
"""
test3 = [["a", "b", "a", "c", "b", "a"], ["bb", "aa", "bb", "aa", "cc"]]


def count_freq(tokens: list[str]) -> dict[str, int]:
    alf = list(sorted(set(tokens)))
    chastots = {}
    for el in alf:
        chastots[el] = tokens.count(el)
    return chastots


"""
for el in test3:
    print(f'{repr(el)} -> {repr(count_freq(el))}')
"""
test4 = [{"a": 3, "b": 2, "c": 1}, {"aa": 2, "bb": 2, "cc": 1}]


def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    top_dict = dict(sorted(freq.items(), key=lambda item: (-item[1], item[0])))
    top = list(top_dict.items())[:n]
    return top

"""
for el in test3:
    print(f"{repr(el)} -> {repr(top_n(count_freq(el)))}")
"""

def all_for_top_n(text: str, n: int = 5) -> list[tuple[str, int]]:
    return top_n(count_freq(tokenize(normalize(text))), n)


for el in test1:
    print(f'{repr(el)} -> {repr(all_for_top_n(el))}')


"""
# normalize
assert normalize("ÐŸÑ€Ð˜Ð²Ð•Ñ‚\nÐœÐ˜Ñ€\t") == "Ð¿Ñ€Ð¸Ð²ÐµÑ‚ Ð¼Ð¸Ñ€"
assert normalize("Ñ‘Ð¶Ð¸Ðº, ÐÐ»ÐºÐ°") == "ÐµÐ¶Ð¸Ðº, ÐµÐ»ÐºÐ°"

# tokenize
assert tokenize("Ð¿Ñ€Ð¸Ð²ÐµÑ‚, Ð¼Ð¸Ñ€!") == ["Ð¿Ñ€Ð¸Ð²ÐµÑ‚", "Ð¼Ð¸Ñ€"]
assert tokenize("Ð¿Ð¾-Ð½Ð°ÑÑ‚Ð¾ÑÑ‰ÐµÐ¼Ñƒ ÐºÑ€ÑƒÑ‚Ð¾") == ["Ð¿Ð¾-Ð½Ð°ÑÑ‚Ð¾ÑÑ‰ÐµÐ¼Ñƒ", "ÐºÑ€ÑƒÑ‚Ð¾"]
assert tokenize("2025 Ð³Ð¾Ð´") == ["2025", "Ð³Ð¾Ð´"]

# count_freq + top_n
freq = count_freq(["a","b","a","c","b","a"])
assert freq == {"a":3, "b":2, "c":1}
assert top_n(freq, 2) == [("a",3), ("b",2)]

# Ñ‚Ð°Ð¹-Ð±Ñ€ÐµÐ¹Ðº Ð¿Ð¾ ÑÐ»Ð¾Ð²Ñƒ Ð¿Ñ€Ð¸ Ñ€Ð°Ð²Ð½Ð¾Ð¹ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ðµ
freq2 = count_freq(["bb","aa","bb","aa","cc"])
assert top_n(freq2, 2) == [("aa",2), ("bb",2)]
"""
if __name__ == "__main__":
    test1 = ["ÐŸÑ€Ð˜Ð²Ð•Ñ‚\nÐœÐ˜Ñ€\t", "Ñ‘Ð¶Ð¸Ðº, ÐÐ»ÐºÐ°", "Hello\r\nWorld", "  Ð´Ð²Ð¾Ð¹Ð½Ñ‹Ðµ   Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹  "]
    for el in test1:
        print(f'{repr(el)} -> {repr(all_for_top_n(el))}')